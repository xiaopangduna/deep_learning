import csv
import os
import hashlib
from typing import List, Dict, Optional, Callable, Any
from torch.utils.data import Dataset
from tqdm import tqdm  # ç›´æ¥å¼•å…¥tqdmç”¨äºè¿›åº¦æ˜¾ç¤º
import cv2
import numpy as np
import hashlib
from pathlib import Path


class BaseDataset(Dataset):
    """
    åŸºç¡€æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ªPyTorchçš„Datasetï¼Œç”¨äºåŠ è½½å’Œç®¡ç†å¸¦è·¯å¾„ä¿¡æ¯çš„CSVæ ¼å¼æ•°æ®é›†ã€‚

    ã€æ ¸å¿ƒåŠŸèƒ½ã€‘
    - è§£æCSVä¸­çš„ç»å¯¹/ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„åŸºäºCSVæ‰€åœ¨ç›®å½•è§£æï¼‰
    - åŒºåˆ†æœ‰æ•ˆæ ·æœ¬ï¼ˆå«è´Ÿæ ·æœ¬ï¼‰ä¸æ— æ•ˆæ ·æœ¬ï¼Œä»…ä¿ç•™æœ‰æ•ˆæ ·æœ¬
    - æä¾›å®Œæ•´çš„æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ€»æ•°ã€è´Ÿæ ·æœ¬å æ¯”ã€æ— æ•ˆæ ·æœ¬è¯¦æƒ…ç­‰ï¼‰

    ã€æ ¸å¿ƒå±æ€§ã€‘
    - csv_paths: List[str]ï¼Œè¾“å…¥çš„CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    - key_map: Dict[str, str]ï¼Œç±»å†…å­—æ®µâ†’CSVè¡¨å¤´å­—æ®µçš„æ˜ å°„ï¼ˆå¦‚{"img": "image_path"}ï¼‰
    - transform: Optional[Callable]ï¼Œæ•°æ®å¢å¼ºå®ä¾‹ï¼ˆå¤–éƒ¨ä¼ å…¥ï¼Œéœ€ä¸ºå¯è°ƒç”¨å¯¹è±¡ï¼Œå¦‚ComposeåŒ…è£…çš„å¢å¼ºæµæ°´çº¿ï¼›é»˜è®¤Noneï¼Œå³ä¸å¢å¼ºï¼‰
    - sample_path_table: Dict[str, List[str]]ï¼Œè·¯å¾„è¡¨æ ¼ï¼ˆé”®ä¸ºç±»å†…å­—æ®µï¼Œå€¼ä¸ºè·¯å¾„åˆ—è¡¨ï¼Œç©ºå­—ç¬¦ä¸²è¡¨ç¤ºè´Ÿæ ·æœ¬ï¼‰
    - num_samples: intï¼Œæœ‰æ•ˆæ ·æœ¬æ€»æ•°ï¼ˆå«è´Ÿæ ·æœ¬ï¼‰

    ã€æ ¸å¿ƒæ–¹æ³•ã€‘
    - __init__: åˆå§‹åŒ–æ•°æ®é›†ï¼Œå®Œæˆè¾“å…¥éªŒè¯ã€è·¯å¾„è§£æå’Œæ ·æœ¬è®¡æ•°
    - __getitem__: é€šè¿‡ç´¢å¼•è·å–æ ·æœ¬å®¹å™¨ï¼ˆå­—å…¸ï¼Œé”®ä¸º"ç±»å†…å­—æ®µ_path"ï¼‰
    - __len__: è¿”å›æœ‰æ•ˆæ ·æœ¬æ€»æ•°
    - __str__: æ‰“å°æ•°æ®é›†å®Œæ•´ç»Ÿè®¡ä¿¡æ¯ï¼ˆCSVåˆ—è¡¨ã€å­—æ®µæ˜ å°„ã€æ ·æœ¬ç»Ÿè®¡ç­‰ï¼‰

    ã€ç”¨æ³•ç¤ºä¾‹ã€‘
    >>> # 1. å®šä¹‰å­—æ®µæ˜ å°„ï¼ˆç±»å†…å­—æ®µâ†’CSVè¡¨å¤´ï¼‰
    >>> key_map = {"img": "image_path", "label": "label_path"}
    >>> # 2. å®ä¾‹åŒ–æ•°æ®é›†
    >>> dataset = BaseDataset(csv_paths=["train.csv"], key_map=key_map)
    >>> # 3. æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    >>> print(dataset)
    >>> # 4. è®¿é—®æ ·æœ¬ï¼ˆè¿”å›{"img_path": "...", "label_path": "..."}ï¼‰
    >>> sample = dataset[0]

    ã€æœ¯è¯­è¯´æ˜ã€‘
    - è´Ÿæ ·æœ¬ï¼šåŒ…å«ç©ºè·¯å¾„ï¼ˆ""ï¼‰çš„æœ‰æ•ˆæ ·æœ¬ï¼ˆå¦‚label_pathä¸ºç©ºï¼Œè¡¨ç¤ºæ— æ ‡ç­¾çš„è´Ÿæ ·æœ¬ï¼‰
    - æ— æ•ˆæ ·æœ¬ï¼šåŒ…å«éç©ºä½†å®é™…ä¸å­˜åœ¨çš„è·¯å¾„çš„æ ·æœ¬ï¼ˆä¼šè¢«è¿‡æ»¤ï¼Œä¸çº³å…¥æœ‰æ•ˆæ ·æœ¬ï¼‰
    """

    def __init__(self, csv_paths: List[str], key_map: Dict[str, str], transform: Optional[Callable] = None):
        # æ ¸å¿ƒé…ç½®å‚æ•°
        self.csv_paths = csv_paths  # CSVæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        self.key_map = key_map  # ç±»å†…å­—æ®µâ†’CSVè¡¨å¤´å­—æ®µçš„æ˜ å°„
        self.transform = transform  # 

        # æ ¸å¿ƒæ•°æ®
        self.sample_path_table: Dict[str, List[str]] = {}  # å­˜å‚¨è§£æåçš„ç»å¯¹è·¯å¾„
        self.num_samples: int = 0

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats_invalid_records: List[str] = []  # è®°å½•æ— æ•ˆè·¯å¾„è¯¦æƒ…
        self._stats_negative_samples: int = 0  # æ–°å¢ï¼šè´Ÿæ ·æœ¬è®¡æ•°ï¼ˆå«ç©ºè·¯å¾„çš„æ ·æœ¬ï¼‰
        # åˆå§‹åŒ–
        self._validate_inputs()
        self.sample_path_table = self._generate_sample_path_table()
        self.num_samples = self._count_and_validate_samples()

    def _validate_inputs(self) -> None:
        """éªŒè¯è¾“å…¥çš„CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        if not isinstance(self.csv_paths, list) or len(self.csv_paths) == 0:
            raise ValueError("âŒ csv_pathså¿…é¡»æ˜¯è‡³å°‘åŒ…å«1ä¸ªæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨")

        for path in self.csv_paths:
            abs_path = os.path.abspath(path)
            if not os.path.isfile(abs_path):
                raise FileNotFoundError(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}ï¼ˆè§£æä¸ºç»å¯¹è·¯å¾„ï¼š{abs_path}ï¼‰")

        if not isinstance(self.key_map, dict) or len(self.key_map) == 0:
            raise ValueError("âŒ key_mapå¿…é¡»æ˜¯è‡³å°‘åŒ…å«1ä¸ªé”®å€¼å¯¹çš„å­—å…¸")

        # æ£€æŸ¥CSVè¡¨å¤´å­—æ®µæ˜¯å¦é‡å¤
        csv_fields = list(self.key_map.values())
        if len(csv_fields) != len(set(csv_fields)):
            duplicates = [f for f in set(csv_fields) if csv_fields.count(f) > 1]
            raise ValueError(f"âŒ key_mapä¸­CSVè¡¨å¤´å­—æ®µé‡å¤ï¼š{duplicates}")

    def _generate_sample_path_table(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆè·¯å¾„è¡¨æ ¼ï¼Œæ­£ç¡®å¤„ç†ç»å¯¹è·¯å¾„ã€ç›¸å¯¹è·¯å¾„å’Œç©ºè·¯å¾„ï¼ˆè´Ÿæ ·æœ¬ï¼‰"""
        path_table = {inner_field: [] for inner_field in self.key_map.keys()}
        csv_fields = list(self.key_map.values())

        for csv_path in self.csv_paths:
            # 1. è§£æCSVæ–‡ä»¶çš„ç»å¯¹è·¯å¾„å’Œæ‰€åœ¨ç›®å½•
            abs_csv_path = os.path.abspath(csv_path)
            csv_dir = os.path.dirname(abs_csv_path)  # ç›¸å¯¹è·¯å¾„çš„åŸºå‡†ç›®å½•

            with open(abs_csv_path, "r", encoding="utf-8", newline="") as f:
                reader = csv.DictReader(f)

                # æ£€æŸ¥CSVæ˜¯å¦åŒ…å«æ‰€éœ€å­—æ®µ
                missing_fields = [f for f in csv_fields if f not in reader.fieldnames]
                if missing_fields:
                    raise ValueError(f"âŒ CSV {abs_csv_path} ç¼ºå°‘å¿…éœ€å­—æ®µï¼š{missing_fields}")

                # 2. é€è¡Œå¤„ç†è·¯å¾„
                for row_idx, row in enumerate(reader, start=2):  # è¡Œå·ä»2å¼€å§‹ï¼ˆè¡¨å¤´ä¸º1ï¼‰
                    current_row = {}
                    valid = True
                    error_details = []
                    has_empty_path = False  # æ ‡è®°å½“å‰è¡Œæ˜¯å¦åŒ…å«ç©ºè·¯å¾„ï¼ˆè´Ÿæ ·æœ¬ï¼‰

                    for inner_field, csv_field in self.key_map.items():
                        # è¯»å–åŸå§‹è·¯å¾„
                        raw_path = row[csv_field].strip()
                        if not raw_path:
                            # ç©ºè·¯å¾„ï¼šè§†ä¸ºè´Ÿæ ·æœ¬ç‰¹å¾ï¼Œä¿ç•™ç©ºå­—ç¬¦ä¸²
                            current_row[inner_field] = ""
                            has_empty_path = True
                            continue  # ç©ºè·¯å¾„ä¸å½±å“æ ·æœ¬æœ‰æ•ˆæ€§

                        # 3. è·¯å¾„è§£ææ ¸å¿ƒé€»è¾‘ï¼ˆéç©ºè·¯å¾„ï¼‰
                        if os.path.isabs(raw_path):
                            resolved_path = raw_path
                        else:
                            resolved_path = os.path.join(csv_dir, raw_path)
                            resolved_path = os.path.abspath(resolved_path)

                        # 4. æ£€æŸ¥è·¯å¾„æœ‰æ•ˆæ€§ï¼ˆéç©ºè·¯å¾„å¿…é¡»å­˜åœ¨ï¼‰
                        current_row[inner_field] = resolved_path
                        if not os.path.exists(resolved_path):
                            valid = False
                            error_details.append(
                                f"å­—æ®µ[{csv_field}]è·¯å¾„ä¸å­˜åœ¨ï¼ˆåŸå§‹è·¯å¾„ï¼š{raw_path}ï¼Œè§£æåï¼š{resolved_path}ï¼‰"
                            )

                    # 5. å¤„ç†å½“å‰è¡Œç»“æœ
                    if valid:
                        # æ‰€æœ‰éç©ºè·¯å¾„éƒ½æœ‰æ•ˆï¼šæ·»åŠ åˆ°è¡¨æ ¼
                        for field, path in current_row.items():
                            path_table[field].append(path)
                        # ç»Ÿè®¡è´Ÿæ ·æœ¬ï¼ˆåŒ…å«ç©ºè·¯å¾„çš„æœ‰æ•ˆæ ·æœ¬ï¼‰
                        if has_empty_path:
                            self._stats_negative_samples += 1
                    else:
                        # å­˜åœ¨æ— æ•ˆè·¯å¾„ï¼ˆéç©ºä¸”ä¸å­˜åœ¨ï¼‰ï¼šè®°å½•å¹¶è·³è¿‡
                        self._stats_invalid_records.append(
                            f"CSV {abs_csv_path} ç¬¬{row_idx}è¡Œï¼š{'; '.join(error_details)}"
                        )

        # æ‰“å°è·³è¿‡çš„æ ·æœ¬ç»Ÿè®¡
        total_skipped = len(self._stats_invalid_records)
        if total_skipped > 0:
            print(f"âš ï¸  å…±è·³è¿‡{total_skipped}ä¸ªæ— æ•ˆæ ·æœ¬")

        return path_table

    def _count_and_validate_samples(self) -> int:
        """éªŒè¯æ‰€æœ‰å­—æ®µçš„æ ·æœ¬æ•°é‡æ˜¯å¦ä¸€è‡´"""
        if not self.sample_path_table:
            return 0

        field_lengths = [len(paths) for paths in self.sample_path_table.values()]
        if len(set(field_lengths)) != 1:
            raise ValueError(f"âŒ å­—æ®µæ ·æœ¬æ•°ä¸åŒ¹é…ï¼š{field_lengths}")

        return field_lengths[0]

    def __len__(self) -> int:
        return self.num_samples

    def __getitem__(self, index: int) -> Dict[str, str]:
        if not 0 <= index < self.num_samples:
            raise IndexError(f"âŒ ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼ˆæœ‰æ•ˆèŒƒå›´ï¼š0~{self.num_samples - 1}ï¼‰")

        # ç”Ÿæˆæ ·æœ¬ä¿¡æ¯å®¹å™¨ï¼šé”®ä¸ºkey_mapçš„ç±»å†…å­—æ®µ+"_path"ï¼Œå€¼ä¸ºå¯¹åº”çš„ç»å¯¹è·¯å¾„
        sample_container = {
            f"{inner_field}": self.sample_path_table[inner_field][index] for inner_field in self.key_map.keys()
        }

        return sample_container

    def __str__(self) -> str:
        """
        é­”æ³•å‡½æ•°ï¼šprint(dataset) æ—¶è‡ªåŠ¨è°ƒç”¨ï¼Œè¾“å‡ºå®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯
        åŒ…å«CSVæ–‡ä»¶ä¿¡æ¯ã€å­—æ®µæ˜ å°„ã€æ ·æœ¬æ•°é‡åŠæ— æ•ˆæ ·æœ¬è¯¦æƒ…
        """
        # è®¡ç®—è´Ÿæ ·æœ¬å æ¯”
        negative_ratio = (self._stats_negative_samples / self.num_samples * 100) if self.num_samples > 0 else 0

        lines = [
            "=" * 70,
            "ğŸ“Š BaseDataset å®Œæ•´ç»Ÿè®¡ä¿¡æ¯",
            "-" * 70,
        ]

        # 1. CSVæ–‡ä»¶ä¿¡æ¯
        lines.append(f"1. åŠ è½½çš„CSVæ–‡ä»¶ï¼ˆå…±{len(self.csv_paths)}ä¸ªï¼‰ï¼š")
        for i, path in enumerate(self.csv_paths, 1):
            lines.append(f"   {i}. è·¯å¾„ï¼š{path}")

        # 2. å­—æ®µæ˜ å°„å…³ç³»
        lines.append(f"\n2. å­—æ®µæ˜ å°„å…³ç³»ï¼ˆå…±{len(self.key_map)}ä¸ªï¼‰ï¼š")
        for inner_field, csv_field in self.key_map.items():
            lines.append(f"   ç±»å†…å­—æ®µ[{inner_field}] â†’ CSVè¡¨å¤´[{csv_field}]")

        # 3. æ ·æœ¬æ•°é‡ç»Ÿè®¡ï¼ˆæ–°å¢è´Ÿæ ·æœ¬ä¿¡æ¯ï¼‰
        lines.extend(
            [
                f"\n3. æ ·æœ¬æ•°é‡ç»Ÿè®¡ï¼š",
                f"   æœ‰æ•ˆæ ·æœ¬æ€»æ•°ï¼ˆå«è´Ÿæ ·æœ¬ï¼‰ï¼š{self.num_samples}",
                f"   è´Ÿæ ·æœ¬æ•°ï¼ˆå«ç©ºè·¯å¾„ï¼‰ï¼š{self._stats_negative_samples}ï¼ˆ{negative_ratio:.1f}%ï¼‰",
                f"   æ— æ•ˆæ ·æœ¬æ€»æ•°ï¼ˆéç©ºè·¯å¾„ä¸å­˜åœ¨ï¼‰ï¼š{len(self._stats_invalid_records)}",
            ]
        )

        # 4. æ— æ•ˆæ ·æœ¬è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ï¼‰
        if self._stats_invalid_records:
            lines.append(f"\n4. æ— æ•ˆæ ·æœ¬è¯¦æƒ…ï¼ˆå…±{len(self._stats_invalid_records)}ä¸ªï¼‰ï¼š")
            for i, err in enumerate(self._stats_invalid_records, 1):
                lines.append(f"   {i}. {err}")
        else:
            lines.append("\n4. æ— æ•ˆæ ·æœ¬è¯¦æƒ…ï¼šæ— ")

        lines.append("=" * 70)
        return "\n".join(lines)

    @staticmethod
    def cache_image(img_paths: List[str], cache_dir: str) -> List[str]:
        """
        ç”Ÿæˆå›¾åƒç¼“å­˜ï¼ˆä»…åŠ é€Ÿè¯»å–ï¼Œä¸åšä»»ä½•é¢„å¤„ç†ï¼‰

        åŠŸèƒ½ï¼š
            å°†åŸå§‹å›¾åƒä»¥.npyæ ¼å¼ä¿å­˜åˆ°æŒ‡å®šç›®å½•ï¼Œé€šè¿‡å›¾åƒå†…å®¹å“ˆå¸Œç¡®ä¿ç¼“å­˜å”¯ä¸€æ€§ï¼Œ
            å·²å­˜åœ¨çš„ç¼“å­˜ä¼šè¢«å¤ç”¨ï¼Œæœ€ç»ˆè¿”å›ä¸è¾“å…¥å›¾åƒè·¯å¾„é¡ºåºä¸€è‡´çš„ç¼“å­˜è·¯å¾„åˆ—è¡¨ã€‚

        ç¼“å­˜æ–‡ä»¶åè§„åˆ™ï¼š
            é‡‡ç”¨å›¾åƒå†…å®¹MD5å“ˆå¸Œå‰16ä½ + ".npy"æ ¼å¼ï¼Œä¾‹å¦‚ï¼š
            "a1b2c3d4e5f6g7h8.npy"ï¼Œä¾¿äºåŒºåˆ†å›¾åƒç¼“å­˜ä¸å…¶ä»–ç±»å‹æ–‡ä»¶ã€‚

        å‚æ•°ï¼š
            img_paths: List[str] - åŸå§‹å›¾åƒè·¯å¾„åˆ—è¡¨ï¼ˆå®Œæ•´è·¯å¾„ï¼‰
            cache_dir: str - ç¼“å­˜æ–‡ä»¶ä¿å­˜ç›®å½•ï¼ˆä¸å­˜åœ¨æ—¶è‡ªåŠ¨åˆ›å»ºï¼‰

        è¿”å›ï¼š
            List[str] - ä¸img_pathsé¡ºåºå¯¹åº”çš„.npyç¼“å­˜è·¯å¾„åˆ—è¡¨

        å¼‚å¸¸ï¼š
            FileNotFoundError: å½“è¾“å…¥å›¾åƒè·¯å¾„ä¸å­˜åœ¨æˆ–æ— æ³•è¯»å–æ—¶æŠ›å‡º
        """
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(cache_dir, exist_ok=True)

        # å­˜å‚¨ç¼“å­˜è·¯å¾„ï¼Œä¸è¾“å…¥å›¾åƒè·¯å¾„é¡ºåºä¸¥æ ¼ä¸€è‡´
        npy_paths = []

        # éå†æ‰€æœ‰å›¾åƒè·¯å¾„ï¼Œå¸¦è¿›åº¦æ¡æ˜¾ç¤º
        for img_path in tqdm(img_paths, desc="ç”Ÿæˆå›¾åƒç¼“å­˜", unit="å¼ "):
            # ç”Ÿæˆå›¾åƒå†…å®¹çš„MD5å“ˆå¸Œï¼ˆç¡®ä¿ç›¸åŒå›¾åƒå¤ç”¨ç¼“å­˜ï¼‰
            with open(img_path, "rb") as f:
                img_content = f.read()
            img_hash = hashlib.md5(img_content).hexdigest()[:16]  # å–å‰16ä½å“ˆå¸Œå€¼

            # æ„å»ºç¼“å­˜æ–‡ä»¶åå’Œå®Œæ•´è·¯å¾„
            npy_filename = f"{img_hash}.npy"
            npy_path = os.path.join(cache_dir, npy_filename)

            # è‹¥ç¼“å­˜ä¸å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆï¼ˆä»…è¯»å–åŸå›¾ï¼Œä¸åšä»»ä½•é¢„å¤„ç†ï¼‰
            if not os.path.exists(npy_path):
                # è¯»å–åŸå§‹å›¾åƒï¼ˆä¿ç•™OpenCVé»˜è®¤çš„BGRé€šé“é¡ºåºï¼‰
                img = cv2.imread(img_path)
                if img is None:
                    raise FileNotFoundError(f"æ— æ³•è¯»å–å›¾åƒæ–‡ä»¶ï¼ˆè·¯å¾„ä¸å­˜åœ¨æˆ–æ–‡ä»¶æŸåï¼‰ï¼š{img_path}")

                # ç›´æ¥ä¿å­˜åŸå§‹å›¾åƒæ•°æ®ï¼ˆä¸åšé€šé“è½¬æ¢ã€resizeç­‰ä»»ä½•æ“ä½œï¼‰
                np.save(npy_path, img)

            # è®°å½•å½“å‰å›¾åƒçš„ç¼“å­˜è·¯å¾„ï¼Œä¿æŒä¸è¾“å…¥é¡ºåºä¸€è‡´
            npy_paths.append(npy_path)

        return npy_paths

    @staticmethod
    def get_hash(obj: Any) -> str:
        """ç”Ÿæˆä»»æ„å¯¹è±¡çš„MD5å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜æ ¡éªŒ"""
        hash_obj = hashlib.md5()

        if isinstance(obj, (list, tuple)):
            # å¯¹åˆ—è¡¨/å…ƒç»„ï¼Œæ’åºååºåˆ—åŒ–ï¼ˆç¡®ä¿é¡ºåºä¸å½±å“å“ˆå¸Œï¼‰
            for item in sorted(obj):
                hash_obj.update(str(item).encode("utf-8"))
        elif isinstance(obj, dict):
            # å¯¹å­—å…¸ï¼ŒæŒ‰é”®æ’åºååºåˆ—åŒ–
            for key in sorted(obj):
                hash_obj.update(f"{key}:{obj[key]}".encode("utf-8"))
        elif isinstance(obj, (str, Path)):
            # å¯¹æ–‡ä»¶è·¯å¾„ï¼Œè¯»å–æ–‡ä»¶å†…å®¹ç”Ÿæˆå“ˆå¸Œ
            obj = Path(obj)
            if obj.exists() and obj.is_file():
                # è¯»å–å‰1MBå†…å®¹ï¼ˆå¹³è¡¡æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼‰
                with open(obj, "rb") as f:
                    while chunk := f.read(1024 * 1024):
                        hash_obj.update(chunk)
                        break  # ä»…è¯»1MB
        else:
            # å…¶ä»–ç±»å‹ç›´æ¥åºåˆ—åŒ–
            hash_obj.update(str(obj).encode("utf-8"))

        return hash_obj.hexdigest()


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•

    # å‡è®¾CSVæ–‡ä»¶å’Œæ•°æ®æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹ï¼š
    # dataset/
    #   â”œâ”€ data.csv
    #   â”œâ”€ images/
    #   â”‚   â”œâ”€ 001.jpg
    #   â”‚   â””â”€ 002.jpg
    #   â””â”€ labels/
    #       â”œâ”€ 001.txt
    #       â””â”€ 002.txt

    CSV_FILES = [
        "/home/xiaopangdun/project/deep_learning/src/train/datasets/coco8/train.csv"
    ]  # å¯ä»¥æ˜¯ç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„
    FIELD_MAP = {
        "img_path": "data_img",  # ç±»å†…å­—æ®µimgå¯¹åº”CSVä¸­çš„image_pathåˆ—
        "label_path": "label_detect_yolo",  # ç±»å†…å­—æ®µlabelå¯¹åº”CSVä¸­çš„label_pathåˆ—
    }

    dataset = BaseDataset(csv_paths=CSV_FILES, key_map=FIELD_MAP)

    dataset.cache_image(dataset.sample_path_table["img"], "cache")
    print(dataset)
