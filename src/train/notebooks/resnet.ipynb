{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09cb85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 定义基础残差块\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # 输出通道扩展倍数（Bottleneck 会用 4）\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 卷积层 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 卷积层 2\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # 下采样（如果输入输出维度不一致，需要调整 shortcut）\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # 保存输入\n",
    "\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # 如果需要调整输入维度，走 downsample\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity  # 残差连接\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 2. 定义 ResNet 主体\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        \"\"\"\n",
    "        block: 残差块类型（BasicBlock 或 Bottleneck）\n",
    "        layers: 每个 stage 的 block 数量，例如 [2,2,2,2] 对应 ResNet-18\n",
    "        num_classes: 分类类别数\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # stem\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # 4 个 stage\n",
    "        self.layer1 = self._make_layer(block, 64,  layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # 全局平均池化 + 全连接层\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride):\n",
    "        \"\"\"\n",
    "        构建一个 stage（包含多个残差块）\n",
    "        out_channels: 输出通道数\n",
    "        blocks: 残差块数量\n",
    "        stride: 第一个 block 的 stride（是否下采样）\n",
    "        \"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 3. 构建 ResNet-18\n",
    "def resnet18(num_classes=1000):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "\n",
    "# 测试网络\n",
    "if __name__ == \"__main__\":\n",
    "    model = resnet18(num_classes=10)\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    y = model(x)\n",
    "    print(y.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc: 1000  # 主任务分类数\n",
    "\n",
    "stem:\n",
    "  - {from: [0], module: Conv, args: {out_channels: 64, kernel_size: 7, stride: 2, padding: 3}}\n",
    "  - {from: [-1], module: BatchNorm, args: {}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool, args: {kernel_size: 3, stride: 2, padding: 1}}\n",
    "\n",
    "backbone:\n",
    "  # Stage 1\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 64, stride: 1, dilation: 1}}\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 64, stride: 1, dilation: 1}}\n",
    "  # Stage 2\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 128, stride: 2, dilation: 1}}\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 128, stride: 1, dilation: 1}}\n",
    "  # Stage 3\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 256, stride: 2, dilation: 1}}\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 256, stride: 1, dilation: 1}}\n",
    "  # Stage 4\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 512, stride: 2, dilation: 1}}\n",
    "  - {from: [-1], module: ResBlock, args: {channels: 512, stride: 1, dilation: 1}}\n",
    "\n",
    "head:\n",
    "  - {from: [-1], module: GlobalAvgPool, args: {}}\n",
    "  - {from: [-1], module: FC, args: {out_features: 1000}}  # 主任务分类头\n",
    "  - {from: [-2], module: FC, args: {out_features: 10}}    # 辅助任务分类头\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c830e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "# ------------------------------\n",
    "# 基础模块\n",
    "# ------------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, stride=1, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 模块工厂\n",
    "# ------------------------------\n",
    "def module_factory(module_name, args, prev_out=None):\n",
    "    modules = {\n",
    "        'Conv': lambda: nn.Conv2d(**args),\n",
    "        'BatchNorm': lambda: nn.BatchNorm2d(args.get('num_features', prev_out.shape[1])),\n",
    "        'ReLU': lambda: nn.ReLU(inplace=True),\n",
    "        'MaxPool': lambda: nn.MaxPool2d(**args),\n",
    "        'GlobalAvgPool': lambda: nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        'FC': lambda: nn.Linear(args.get('in_features', prev_out.shape[1]), args['out_features']),\n",
    "        'ResBlock': lambda: ResBlock(**args)\n",
    "    }\n",
    "    if module_name not in modules:\n",
    "        raise ValueError(f\"Unknown module: {module_name}\")\n",
    "    return modules[module_name]()\n",
    "\n",
    "# ------------------------------\n",
    "# 支持多输入多输出的网络\n",
    "# ------------------------------\n",
    "class YAMLNet(nn.Module):\n",
    "    def __init__(self, yaml_path, in_channels=3, verbose=True):\n",
    "        super().__init__()\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            self.cfg = yaml.safe_load(f)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.from_list = []\n",
    "        chs = [in_channels]\n",
    "\n",
    "        # 构建层\n",
    "        for layer_cfg in self.cfg.get('stem', []) + self.cfg.get('backbone', []) + self.cfg.get('head', []):\n",
    "            f = layer_cfg['from']  # 列表形式\n",
    "            m_name = layer_cfg['module']\n",
    "            args = layer_cfg['args'].copy()\n",
    "            prev_out = None if len(f)==0 else torch.zeros(1, chs[f[0]], 1, 1)  # 临时推测通道\n",
    "            layer = module_factory(m_name, args, prev_out)\n",
    "            self.layers.append(layer)\n",
    "            self.from_list.append(f)\n",
    "\n",
    "            # 更新输出通道\n",
    "            if 'out_channels' in args:\n",
    "                c2 = args['out_channels']\n",
    "            elif 'channels' in args:\n",
    "                c2 = args['channels']\n",
    "            elif 'out_features' in args:\n",
    "                c2 = args['out_features']\n",
    "            else:\n",
    "                c2 = chs[-1]\n",
    "            chs.append(c2)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{len(self.layers)-1:03}: {m_name}, from {f}, args={args}, out_ch={c2}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {0: x}\n",
    "        out_heads = []\n",
    "        head_start_idx = len(self.layers) - len(self.cfg['head'])\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            f = self.from_list[i]\n",
    "            inp = torch.cat([outputs[j] for j in f], dim=1) if len(f) > 1 else outputs[f[0]]\n",
    "            out = layer(inp)\n",
    "            outputs[i+1] = out\n",
    "\n",
    "            # 收集 head 输出\n",
    "            if i >= head_start_idx:\n",
    "                out_heads.append(out)\n",
    "\n",
    "        return tuple(out_heads) if len(out_heads) > 1 else out_heads[0]\n",
    "\n",
    "# ------------------------------\n",
    "# 测试\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = YAMLNet(\"resnet18_multi_task.yaml\", in_channels=3, verbose=True)\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    y = model(x)\n",
    "    if isinstance(y, tuple):\n",
    "        for i, out in enumerate(y):\n",
    "            print(f\"Output head {i} shape:\", out.shape)\n",
    "    else:\n",
    "        print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc: 1000  # 分类数\n",
    "\n",
    "stem:\n",
    "  - {type: \"image\", channels: 3, height: 224, width: 224}  # 图像输入\n",
    "  - {type: \"vector\", channels: 128}                        # 音频特征向量\n",
    "\n",
    "backbone:\n",
    "  # Stage 1 图像处理\n",
    "  - {from: [0], module: Conv, args: {out_channels: 64, kernel_size: 7, stride: 2, padding: 3}}\n",
    "  - {from: [-1], module: BatchNorm, args: {}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool, args: {kernel_size: 3, stride: 2, padding: 1}}\n",
    "\n",
    "  # Stage 2 融合音频\n",
    "  - {from: [-1, 1], module: ResBlock, args: {channels: 128, stride: 1, dilation: 1}}\n",
    "\n",
    "head:\n",
    "  - {from: [-1], module: FC, args: {out_features: 1000}}  # 最终输出层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a795c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc: 1000  # 分类数\n",
    "\n",
    "stem:\n",
    "  - {type: \"image\", channels: 3, height: 224, width: 224}  # 图像输入\n",
    "  - {type: \"vector\", channels: 128}                        # 音频特征向量\n",
    "\n",
    "backbone:\n",
    "  # Stage 1 图像处理\n",
    "  - {from: [0], module: Conv, args: {out_channels: 64, kernel_size: 7, stride: 2, padding: 3}}\n",
    "  - {from: [-1], module: BatchNorm, args: {}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool, args: {kernel_size: 3, stride: 2, padding: 1}}\n",
    "\n",
    "  # Stage 2 融合音频\n",
    "  - {from: [-1, 1], module: ResBlock, args: {channels: 128, stride: 1, dilation: 1}}\n",
    "\n",
    "head:\n",
    "  - {from: [-1], module: FC, args: {out_features: 1000}}  # 最终输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e55413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "# ----------------------------\n",
    "# 简单 ResBlock 示例\n",
    "# ----------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels, stride=1, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1,\n",
    "                               padding=dilation, dilation=dilation, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ----------------------------\n",
    "# 简单 FC 层，可包含 flatten\n",
    "# ----------------------------\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=None, out_features=out_features)  # in_features 动态设置\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim > 2:\n",
    "            x = torch.flatten(x, 1)\n",
    "        # 动态初始化 in_features\n",
    "        if self.fc.in_features is None:\n",
    "            self.fc = nn.Linear(x.shape[1], self.out_features).to(x.device)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ----------------------------\n",
    "# 模型构建函数\n",
    "# ----------------------------\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        layers_cfg = cfg['backbone'] + cfg['head']\n",
    "        self.from_list = [l['from'] for l in layers_cfg]\n",
    "        self.layers = nn.ModuleList()\n",
    "        for l in layers_cfg:\n",
    "            module_cls = globals()[l['module']]\n",
    "            self.layers.append(module_cls(**l['args']))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: list of input tensors, 对应 stem\n",
    "        outputs = {i: inp for i, inp in enumerate(inputs)}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            f = self.from_list[i]\n",
    "            f = f if isinstance(f, list) else [f]\n",
    "            x = torch.cat([outputs[j] for j in f], dim=1) if len(f) > 1 else outputs[f[0]]\n",
    "            out = layer(x)\n",
    "            outputs[len(outputs)] = out\n",
    "        return out  # 只返回 head 最终输出\n",
    "\n",
    "# ----------------------------\n",
    "# 测试\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载 YAML\n",
    "    with open(\"resnet_multiinput.yaml\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # 创建模型\n",
    "    model = DynamicNet(cfg)\n",
    "\n",
    "    # 假设输入：图像 + 音频向量\n",
    "    img = torch.randn(2, 3, 224, 224)\n",
    "    audio = torch.randn(2, 128, 56, 56)  # 映射到图像 feature map 尺寸\n",
    "    out = model([img, audio])\n",
    "\n",
    "    print(\"输出 shape:\", out.shape)  # 应为 [2, 1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd208b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc: [10, 5]  # 两个任务的类别数\n",
    "\n",
    "stem:\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入图像1\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入图像2\n",
    "\n",
    "backbone:\n",
    "  # 对图像1处理\n",
    "  - {from: [0], module: Conv2d, args: {in_channels: 3, out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool2d, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "  # 对图像2处理\n",
    "  - {from: [1], module: Conv2d, args: {in_channels: 3, out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool2d, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "head:\n",
    "  - {from: [2], module: LinearDyn, args: {out_features: 10}}  # 输出1\n",
    "  - {from: [5], module: LinearDyn, args: {out_features: 5}}   # 输出2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem:\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入图像1\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入图像2\n",
    "\n",
    "backbone:\n",
    "  - {from: [0], module: Conv2d, args: {in_channels: 3, out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool2d, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "  - {from: [1], module: Conv2d, args: {in_channels: 3, out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool2d, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "head:\n",
    "  - {from: [2], module: Flatten, args: {}}\n",
    "  - {from: [-1], module: Linear, args: {in_features: 16*16*16, out_features: 10}}  # 输出1\n",
    "  - {from: [5], module: Flatten, args: {}}\n",
    "  - {from: [-1], module: Linear, args: {in_features: 16*16*16, out_features: 5}}   # 输出2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5639dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "def build_model(cfg):\n",
    "    layers_cfg = cfg['backbone'] + cfg['head']\n",
    "    from_list = [l['from'] for l in layers_cfg]\n",
    "    layers = nn.ModuleList()\n",
    "    \n",
    "    for l in layers_cfg:\n",
    "        module_cls = getattr(nn, l['module'])\n",
    "        layers.append(module_cls(**l['args']))\n",
    "    \n",
    "    def forward_fn(inputs):\n",
    "        outputs = {i: inp for i, inp in enumerate(inputs)}\n",
    "        results = []\n",
    "        for i, layer in enumerate(layers):\n",
    "            f = from_list[i]\n",
    "            f = f if isinstance(f, list) else [f]\n",
    "            x = torch.cat([outputs[j] for j in f], dim=1) if len(f) > 1 else outputs[f[0]]\n",
    "            out = layer(x)\n",
    "            outputs[len(outputs)] = out\n",
    "            # 收集 head 输出\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                results.append(out)\n",
    "        return results\n",
    "\n",
    "    model = nn.Module()\n",
    "    model.layers = layers\n",
    "    model.forward = forward_fn\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 测试流程\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"simple_twoinput_minimal.yaml\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    img1 = torch.randn(2, 3, 32, 32)\n",
    "    img2 = torch.randn(2, 3, 32, 32)\n",
    "    out1, out2 = model([img1, img2])\n",
    "\n",
    "    print(\"输出1 shape:\", out1.shape)  # [2, 10]\n",
    "    print(\"输出2 shape:\", out2.shape)  # [2, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e298bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc: [10, 5]  # 两个任务的分类数\n",
    "\n",
    "# stem 定义输入\n",
    "stem:\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入1\n",
    "  - {type: \"image\", channels: 3, height: 32, width: 32}  # 输入2\n",
    "\n",
    "# backbone 定义中间层\n",
    "backbone:\n",
    "  # 输入1\n",
    "  - {from: [0], module: Conv, args: {out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "  # 输入2\n",
    "  - {from: [1], module: Conv, args: {out_channels: 16, kernel_size: 3, stride: 1, padding: 1}}\n",
    "  - {from: [-1], module: ReLU, args: {}}\n",
    "  - {from: [-1], module: MaxPool, args: {kernel_size: 2, stride: 2}}\n",
    "\n",
    "  # 拼接输入1和输入2\n",
    "  - {from: [2,5], module: \"Concat\", args: {dim: 1}}\n",
    "\n",
    "head:\n",
    "  - {from: [-1], module: FC, args: {out_features: 10}}  # 输出1\n",
    "  - {from: [-2], module: FC, args: {out_features: 5}}   # 输出2\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "\n",
    "# ----------------------------\n",
    "# 工厂函数\n",
    "# ----------------------------\n",
    "def module_factory(module_name, args, prev_out=None):\n",
    "    \"\"\"仅生成可训练模块，Concat 不在此处理\"\"\"\n",
    "    if module_name == \"Conv\":\n",
    "        in_ch = prev_out.shape[1] if prev_out is not None else args.get('in_channels')\n",
    "        return nn.Conv2d(in_channels=in_ch, **{k: v for k, v in args.items() if k != 'in_channels'})\n",
    "    \n",
    "    elif module_name == \"BatchNorm\":\n",
    "        num_features = prev_out.shape[1] if prev_out is not None else args['num_features']\n",
    "        return nn.BatchNorm2d(num_features)\n",
    "    \n",
    "    elif module_name == \"ReLU\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    \n",
    "    elif module_name == \"MaxPool\":\n",
    "        return nn.MaxPool2d(**args)\n",
    "    \n",
    "    elif module_name == \"GlobalAvgPool\":\n",
    "        return nn.AdaptiveAvgPool2d((1, 1))\n",
    "    \n",
    "    elif module_name == \"FC\":\n",
    "        in_features = prev_out.numel() // prev_out.shape[0] if prev_out is not None else args['in_features']\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, args['out_features'])\n",
    "        )\n",
    "    \n",
    "    elif module_name == \"ResBlock\":\n",
    "        return ResBlock(**args)  # 自定义模块\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown module: {module_name}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 构建模型\n",
    "# ----------------------------\n",
    "def build_model(cfg):\n",
    "    layers_cfg = cfg['backbone'] + cfg['head']\n",
    "    from_list = [l['from'] for l in layers_cfg]\n",
    "\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers_cfg = layers_cfg\n",
    "            self.from_list = from_list\n",
    "            self.layers = nn.ModuleList()\n",
    "            for cfg in layers_cfg:\n",
    "                if cfg['module'] != 'Concat':\n",
    "                    # 用 Identity 占位，真实层在 forward 动态生成\n",
    "                    self.layers.append(nn.Identity())\n",
    "                else:\n",
    "                    self.layers.append(None)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            outputs = {i: inp for i, inp in enumerate(inputs)}\n",
    "            results = []\n",
    "            for i, cfg in enumerate(self.layers_cfg):\n",
    "                f = self.from_list[i]\n",
    "                f = f if isinstance(f, list) else [f]\n",
    "                \n",
    "                # 如果是多输入拼接\n",
    "                if cfg['module'] == 'Concat':\n",
    "                    out = torch.cat([outputs[j] for j in f], dim=cfg['args'].get('dim',1))\n",
    "                else:\n",
    "                    x = torch.cat([outputs[j] for j in f], dim=1) if len(f) > 1 else outputs[f[0]]\n",
    "                    layer = module_factory(cfg['module'], cfg['args'], prev_out=x)\n",
    "                    out = layer(x)\n",
    "                \n",
    "                outputs[len(outputs)] = out\n",
    "                \n",
    "                # 如果是 FC，认为是 head 输出\n",
    "                if cfg['module'] == 'FC':\n",
    "                    results.append(out)\n",
    "            return results\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# ----------------------------\n",
    "# 测试\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"multiinput_multioutput_concat.yaml\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    \n",
    "    model = build_model(cfg)\n",
    "    img1 = torch.randn(2, 3, 32, 32)\n",
    "    img2 = torch.randn(2, 3, 32, 32)\n",
    "    out1, out2 = model([img1, img2])\n",
    "\n",
    "    print(\"输出1 shape:\", out1.shape)  # [2, 10]\n",
    "    print(\"输出2 shape:\", out2.shape)  # [2, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92df02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "def dynamic_class_instantiate_from_string(class_path: str, **kwargs):\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**kwargs)\n",
    "\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, yaml_dict):\n",
    "        super().__init__()\n",
    "        self.yaml_dict = yaml_dict\n",
    "        self.stem_count = len(yaml_dict.get(\"stem\", []))\n",
    "        self.head_idxs = []\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.c_out = []  # 保存每一层的输出通道数\n",
    "        self._init_stem_channels()\n",
    "        self._build_layers()\n",
    "\n",
    "    def _init_stem_channels(self):\n",
    "        \"\"\"把 stem 的输出通道信息加入 c_out\"\"\"\n",
    "        for stem_def in self.yaml_dict.get(\"stem\", []):\n",
    "            self.c_out.append(stem_def[\"channels\"])\n",
    "\n",
    "    def _build_layers(self):\n",
    "        all_defs = self.yaml_dict.get(\"backbone\", []) + self.yaml_dict.get(\"head\", [])\n",
    "        for i, layer_def in enumerate(all_defs):\n",
    "            module_name = layer_def[\"module\"]\n",
    "            args = dict(layer_def.get(\"args\", {}))  # copy dict\n",
    "            from_idxs = layer_def[\"from\"]\n",
    "\n",
    "            # 计算 in_channels，如果模块需要\n",
    "            if module_name != \"Concat\" and \"in_channels\" in dynamic_class_init_params(module_name):\n",
    "                if \"in_channels\" not in args or args[\"in_channels\"] is None:\n",
    "                    if len(from_idxs) == 1:\n",
    "                        args[\"in_channels\"] = self.c_out[from_idxs[0]]\n",
    "                    else:\n",
    "                        # 对于多个输入，拼接维度\n",
    "                        in_ch = sum(self.c_out[idx] for idx in from_idxs)\n",
    "                        args[\"in_channels\"] = in_ch\n",
    "\n",
    "            if module_name != \"Concat\" and module_name != \"FC\":\n",
    "                module = dynamic_class_instantiate_from_string(module_name, **args)\n",
    "                if hasattr(module, \"out_channels\"):\n",
    "                    c2 = module.out_channels\n",
    "                elif hasattr(module, \"weight\"):\n",
    "                    c2 = module.weight.shape[0]\n",
    "                else:\n",
    "                    c2 = None\n",
    "            else:\n",
    "                module = None\n",
    "                if module_name == \"FC\":\n",
    "                    c2 = args.get(\"out_features\")\n",
    "                else:\n",
    "                    c2 = None  # Concat 层在 forward 时计算\n",
    "\n",
    "            self.layers.append(module)\n",
    "            self.c_out.append(c2)\n",
    "\n",
    "            # 记录 head 输出索引\n",
    "            if layer_def[\"module\"] == \"FC\":\n",
    "                self.head_idxs.append(i)\n",
    "\n",
    "\n",
    "def dynamic_class_init_params(class_path):\n",
    "    \"\"\"获取类构造函数参数名\"\"\"\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "    import inspect\n",
    "    return inspect.signature(cls).parameters.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b28c37ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.__init__() missing 1 required positional argument: 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m yaml_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     ]\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 输入张量\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDynamicNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m x1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     26\u001b[0m x2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m, in \u001b[0;36mDynamicNet.__init__\u001b[0;34m(self, yaml_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_out \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# 保存每一层的输出通道数\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stem_channels()\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m, in \u001b[0;36mDynamicNet._build_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m             args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_channels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m in_ch\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m module_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFC\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_class_instantiate_from_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_channels\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m         c2 \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mout_channels\n",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m, in \u001b[0;36mdynamic_class_instantiate_from_string\u001b[0;34m(class_path, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(module_name)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.__init__() missing 1 required positional argument: 'in_features'"
     ]
    }
   ],
   "source": [
    "yaml_dict = {\n",
    "    \"stem\": [\n",
    "        {\"type\": \"image\", \"channels\": 3, \"height\": 32, \"width\": 32},\n",
    "        {\"type\": \"image\", \"channels\": 3, \"height\": 32, \"width\": 32},\n",
    "    ],\n",
    "    \"backbone\": [\n",
    "        {\"from\": [0], \"module\": \"torch.nn.Conv2d\", \"args\": {\"out_channels\": 16, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.ReLU\", \"args\": {}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.MaxPool2d\", \"args\": {\"kernel_size\": 2, \"stride\": 2}},\n",
    "        {\"from\": [1], \"module\": \"torch.nn.Conv2d\", \"args\": {\"out_channels\": 16, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.ReLU\", \"args\": {}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.MaxPool2d\", \"args\": {\"kernel_size\": 2, \"stride\": 2}},\n",
    "        {\"from\": [2, 5], \"module\": \"Concat\", \"args\": {\"dim\": 1}},\n",
    "    ],\n",
    "    \"head\": [\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.Linear\", \"args\": {\"out_features\": 10}},\n",
    "        {\"from\": [-2], \"module\": \"torch.nn.Linear\", \"args\": {\"out_features\": 5}},\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# 输入张量\n",
    "model = DynamicNet(yaml_dict)\n",
    "\n",
    "x1 = torch.randn(1, 3, 32, 32)\n",
    "x2 = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "outs = model([x1, x2])\n",
    "\n",
    "for i, out in enumerate(outs):\n",
    "    print(f\"Head {i} output shape: {out.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "221733ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "def dynamic_class_instantiate_from_string(class_path: str, **kwargs):\n",
    "    \"\"\"根据字符串路径动态实例化类\"\"\"\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**kwargs)\n",
    "\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, yaml_dict):\n",
    "        super().__init__()\n",
    "        self.yaml_dict = yaml_dict\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.c_out = []  # 保存每一层输出通道\n",
    "        self.head_idxs = []\n",
    "\n",
    "        self._init_stem_channels()\n",
    "        self._build_layers()\n",
    "\n",
    "    def _init_stem_channels(self):\n",
    "        stem = self.yaml_dict[\"stem\"]\n",
    "        assert len(stem) == 1, \"当前只支持单输入\"\n",
    "        self.c_out.append(stem[0][\"channels\"])\n",
    "        self.input_shape = (1, stem[0][\"channels\"], stem[0][\"height\"], stem[0][\"width\"])\n",
    "\n",
    "    def _build_layers(self):\n",
    "        # 使用一个虚拟输入推算 Linear 输入尺寸\n",
    "        dummy = torch.randn(self.input_shape)\n",
    "\n",
    "        for i, layer_def in enumerate(self.yaml_dict[\"backbone\"] + self.yaml_dict[\"head\"]):\n",
    "            module_name = layer_def[\"module\"]\n",
    "            from_idxs = layer_def[\"from\"]\n",
    "            args = layer_def.get(\"args\", {})\n",
    "\n",
    "            # 自动填充 in_channels\n",
    "            if module_name in [\"torch.nn.Conv2d\", \"torch.nn.BatchNorm2d\"]:\n",
    "                if \"in_channels\" not in args or args[\"in_channels\"] is None:\n",
    "                    args[\"in_channels\"] = self.c_out[from_idxs[0]]\n",
    "\n",
    "            # 对 Linear 自动填充 in_features\n",
    "            if module_name == \"torch.nn.Linear\":\n",
    "                if \"in_features\" not in args or args[\"in_features\"] is None:\n",
    "                    with torch.no_grad():\n",
    "                        inp = dummy\n",
    "                        out_idxs = from_idxs\n",
    "                        inp = inp if len(out_idxs) == 1 else torch.cat([dummy for _ in out_idxs], dim=1)\n",
    "                        inp_flat = torch.flatten(inp, 1)\n",
    "                        args[\"in_features\"] = inp_flat.shape[1]\n",
    "\n",
    "            module = (\n",
    "                dynamic_class_instantiate_from_string(module_name, **args)\n",
    "                if module_name != \"Concat\"\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            self.layers.append(module)\n",
    "\n",
    "            # 前向更新 dummy\n",
    "            if module is not None:\n",
    "                inp = dummy if len(from_idxs) == 1 else torch.cat([dummy for _ in from_idxs], dim=1)\n",
    "                if isinstance(module, nn.Linear) and inp.ndim > 2:\n",
    "                    inp = torch.flatten(inp, 1)\n",
    "                dummy = module(inp)\n",
    "            else:\n",
    "                # Concat\n",
    "                dummy = torch.cat([dummy for _ in from_idxs], dim=1)\n",
    "\n",
    "            # 更新输出通道\n",
    "            if module_name == \"Concat\":\n",
    "                self.c_out.append(sum(self.c_out[idx] for idx in from_idxs))\n",
    "            elif module_name == \"torch.nn.Linear\":\n",
    "                self.c_out.append(args[\"out_features\"])\n",
    "            elif module_name in [\"torch.nn.Conv2d\", \"torch.nn.BatchNorm2d\"]:\n",
    "                self.c_out.append(args.get(\"out_channels\", self.c_out[from_idxs[0]]))\n",
    "            else:\n",
    "                self.c_out.append(self.c_out[from_idxs[0]])\n",
    "\n",
    "            if i >= len(self.yaml_dict[\"backbone\"]):\n",
    "                self.head_idxs.append(i)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_def = (self.yaml_dict[\"backbone\"] + self.yaml_dict[\"head\"])[i]\n",
    "            from_idxs = layer_def[\"from\"]\n",
    "\n",
    "            if layer is not None:\n",
    "                inp = outputs[from_idxs[0]] if len(from_idxs) == 1 else torch.cat([outputs[idx] for idx in from_idxs], dim=1)\n",
    "                if isinstance(layer, nn.Linear) and inp.ndim > 2:\n",
    "                    inp = torch.flatten(inp, 1)\n",
    "                out = layer(inp)\n",
    "            else:\n",
    "                # Concat\n",
    "                out = torch.cat([outputs[idx] for idx in from_idxs], dim=1)\n",
    "\n",
    "            outputs.append(out)\n",
    "\n",
    "        return outputs[self.head_idxs[0]]\n",
    "\n",
    "# --------------------------\n",
    "# YAML 字典\n",
    "# --------------------------\n",
    "yaml_dict = {\n",
    "    \"stem\": [{\"type\": \"image\", \"channels\": 3, \"height\": 32, \"width\": 32}],\n",
    "    \"backbone\": [\n",
    "        {\"from\": [0], \"module\": \"torch.nn.Conv2d\", \"args\": {\"out_channels\": 16, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.ReLU\", \"args\": {}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.MaxPool2d\", \"args\": {\"kernel_size\": 2, \"stride\": 2}},\n",
    "    ],\n",
    "    \"head\": [\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.Linear\", \"args\": {\"out_features\": 10}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 测试\n",
    "# --------------------------\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "model = DynamicNet(yaml_dict)\n",
    "out = model(x)\n",
    "print(out.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7689005",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     99\u001b[0m model \u001b[38;5;241m=\u001b[39m DynamicNet(yaml_dict)\n\u001b[0;32m--> 100\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# torch.Size([1, 10])\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_deep_learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 63\u001b[0m, in \u001b[0;36mDynamicNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (layer_def, layer) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_def, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m     62\u001b[0m     from_idxs \u001b[38;5;241m=\u001b[39m layer_def\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 63\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [outputs[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m from_idxs]  \u001b[38;5;66;03m# +1 因为 stem 占位\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, dim\u001b[38;5;241m=\u001b[39mlayer_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[17], line 63\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (layer_def, layer) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers_def, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m     62\u001b[0m     from_idxs \u001b[38;5;241m=\u001b[39m layer_def\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 63\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m from_idxs]  \u001b[38;5;66;03m# +1 因为 stem 占位\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     65\u001b[0m         out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(inputs, dim\u001b[38;5;241m=\u001b[39mlayer_def[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "def dynamic_class_instantiate_from_string(class_path: str, **kwargs):\n",
    "    \"\"\"根据字符串路径动态实例化类\"\"\"\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**kwargs)\n",
    "\n",
    "\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, yaml_dict):\n",
    "        super().__init__()\n",
    "        self.yaml_dict = yaml_dict\n",
    "        self.layers_def = yaml_dict.get(\"backbone\", []) + yaml_dict.get(\"head\", [])\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_outputs = []  # 用于存储每层输出通道数或特征数\n",
    "\n",
    "        self._build_layers()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        for i, layer_def in enumerate(self.layers_def):\n",
    "            module_name = layer_def[\"module\"]\n",
    "            args = layer_def.get(\"args\", {}).copy()\n",
    "            from_idxs = layer_def.get(\"from\", [-1])\n",
    "            \n",
    "            # 处理in_channels/in_features\n",
    "            if module_name not in [\"Concat\"]:\n",
    "                if \"in_channels\" in args and args[\"in_channels\"] is None:\n",
    "                    # 线性或卷积层需要推算输入通道\n",
    "                    args[\"in_channels\"] = self.layer_outputs[from_idxs[0]]\n",
    "                if \"in_features\" in args and args[\"in_features\"] is None:\n",
    "                    args[\"in_features\"] = self.layer_outputs[from_idxs[0]]\n",
    "                \n",
    "                layer = dynamic_class_instantiate_from_string(module_name, **args)\n",
    "            else:\n",
    "                layer = None  # Concat 在 forward 中处理\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # 更新 layer_outputs，卷积使用 out_channels，Linear使用 out_features\n",
    "            if module_name == \"Concat\":\n",
    "                ch = sum(self.layer_outputs[idx] for idx in from_idxs)\n",
    "            elif module_name.endswith(\"Conv2d\"):\n",
    "                ch = args[\"out_channels\"]\n",
    "            elif module_name.endswith(\"Linear\"):\n",
    "                ch = args[\"out_features\"]\n",
    "            else:\n",
    "                # 对于 ReLU、MaxPool 等不改变通道数\n",
    "                ch = self.layer_outputs[from_idxs[0]]\n",
    "            self.layer_outputs.append(ch)\n",
    "\n",
    "        # 保存 head 的索引\n",
    "        self.head_idxs = list(range(len(self.layer_outputs) - len(self.yaml_dict.get(\"head\", [])), len(self.layer_outputs)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        outputs.append(x)  # stem 输出\n",
    "        for i, (layer_def, layer) in enumerate(zip(self.layers_def, self.layers)):\n",
    "            from_idxs = layer_def.get(\"from\", [-1])\n",
    "            inputs = [outputs[idx + 1] for idx in from_idxs]  # +1 因为 stem 占位\n",
    "            if layer_def[\"module\"] == \"Concat\":\n",
    "                out = torch.cat(inputs, dim=layer_def[\"args\"][\"dim\"])\n",
    "            else:\n",
    "                inp = inputs[0] if len(inputs) == 1 else torch.cat(inputs, dim=1)\n",
    "                # 如果是 Linear，先 flatten\n",
    "                if isinstance(layer, nn.Linear) and inp.ndim > 2:\n",
    "                    inp = torch.flatten(inp, 1)\n",
    "                out = layer(inp)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        head_outputs = [outputs[idx + 1] for idx in self.head_idxs]\n",
    "        return head_outputs[0] if len(head_outputs) == 1 else head_outputs\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 示例 YAML 字典\n",
    "# --------------------------\n",
    "yaml_dict = {\n",
    "    \"stem\": [\n",
    "        {\"type\": \"image\", \"channels\": 3, \"height\": 32, \"width\": 32}\n",
    "    ],\n",
    "    \"backbone\": [\n",
    "        {\"from\": [0], \"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\": 3, \"out_channels\": 16, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.ReLU\", \"args\": {}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.MaxPool2d\", \"args\": {\"kernel_size\": 2, \"stride\": 2}},\n",
    "    ],\n",
    "    \"head\": [\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.Linear\", \"args\": {\"in_features\": None, \"out_features\": 10}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 测试\n",
    "# --------------------------\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "model = DynamicNet(yaml_dict)\n",
    "out = model(x)\n",
    "print(out.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a6a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import importlib\n",
    "\n",
    "def dynamic_class_instantiate_from_string(class_path: str, **kwargs):\n",
    "    \"\"\"根据字符串路径动态实例化类\"\"\"\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = importlib.import_module(module_name)\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**kwargs)\n",
    "\n",
    "\n",
    "class ModelBuilder(nn.Module):\n",
    "    def __init__(self, yaml_dict):\n",
    "        super().__init__()\n",
    "        self.yaml_dict = yaml_dict\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.output_shapes = {}\n",
    "        self.input_shape = None\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        # 1. stem: 定义输入\n",
    "        stem = self.yaml_dict.get(\"stem\", [])\n",
    "        if stem:\n",
    "            input_cfg = stem[0]\n",
    "            self.input_shape = (input_cfg[\"channels\"], input_cfg[\"height\"], input_cfg[\"width\"])\n",
    "        \n",
    "        # 2. backbone + head\n",
    "        self.parse_layers(\"backbone\")\n",
    "        self.parse_layers(\"head\")\n",
    "\n",
    "    def parse_layers(self, section):\n",
    "        for layer_cfg in self.yaml_dict.get(section, []):\n",
    "            from_idx = layer_cfg.get(\"from\", [-1])[0]\n",
    "            args = dict(layer_cfg[\"args\"])  # 复制，避免修改原 dict\n",
    "            \n",
    "            # 自动推断 in_features\n",
    "            if \"in_features\" in args and args[\"in_features\"] is None:\n",
    "                prev_shape = self.output_shapes[from_idx]\n",
    "                args[\"in_features\"] = prev_shape[0] * prev_shape[1] * prev_shape[2]\n",
    "\n",
    "            # 动态实例化模块\n",
    "            layer = dynamic_class_instantiate_from_string(layer_cfg[\"module\"], **args)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "            # 前向计算一次 shape（dummy input）\n",
    "            dummy_input = torch.zeros((1, *self.input_shape))\n",
    "            for i, l in enumerate(self.layers):\n",
    "                if isinstance(l, nn.Linear):\n",
    "                    dummy_input = dummy_input.view(dummy_input.size(0), -1)\n",
    "                dummy_input = l(dummy_input)\n",
    "                self.output_shapes[i] = dummy_input.shape[1:]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = x.view(x.size(0), -1)\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bab000",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m yaml_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m32\u001b[39m}\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mModelBuilder.__init__\u001b[0;34m(self, yaml_dict)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_shapes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mModelBuilder.build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 2. backbone + head\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_layers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mModelBuilder.parse_layers\u001b[0;34m(self, section)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 自动推断 in_features\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_features\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     prev_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfrom_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prev_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m prev_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m prev_shape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 动态实例化模块\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "yaml_dict = {\n",
    "    \"stem\": [\n",
    "        {\"type\": \"image\", \"channels\": 3, \"height\": 32, \"width\": 32}\n",
    "    ],\n",
    "    \"backbone\": [\n",
    "        {\"from\": [0], \"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\": 3, \"out_channels\": 16, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.ReLU\", \"args\": {}},\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.MaxPool2d\", \"args\": {\"kernel_size\": 2, \"stride\": 2}},\n",
    "    ],\n",
    "    \"head\": [\n",
    "        {\"from\": [-1], \"module\": \"torch.nn.Linear\", \"args\": {\"in_features\": None, \"out_features\": 10}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "model = ModelBuilder(yaml_dict)\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c06e627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def dynamic_class_instantiate_from_string(class_path, **kwargs):\n",
    "    \"\"\"\n",
    "    简单动态实例化类，例如 \"torch.nn.Conv2d\"\n",
    "    \"\"\"\n",
    "    parts = class_path.split(\".\")\n",
    "    module_name, class_name = \".\".join(parts[:-1]), parts[-1]\n",
    "    module = __import__(module_name, fromlist=[class_name])\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls(**kwargs)\n",
    "\n",
    "class DAGNet(nn.Module):\n",
    "    def __init__(self, layers_config, input_name=\"input\"):\n",
    "        \"\"\"\n",
    "        layers_config: dict\n",
    "            key: 层名\n",
    "            value: dict, 包含 'module' 和 'args' 和 'from'\n",
    "            例如：\n",
    "            {\n",
    "                \"conv1\": {\"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\": 3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\": [\"input\"]},\n",
    "                \"relu1\": {\"module\": \"torch.nn.ReLU\", \"args\": {}, \"from\": [\"conv1\"]},\n",
    "                \"conv2\": {\"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\":16, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\": [\"relu1\"]},\n",
    "                \"out\": {\"module\": \"torch.nn.Linear\", \"args\": {\"in_features\":32*32*32, \"out_features\":10}, \"from\": [\"conv2\"]}\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_name = input_name\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.edges = {}  # 保存每层输入\n",
    "\n",
    "        for name, conf in layers_config.items():\n",
    "            self.layers[name] = dynamic_class_instantiate_from_string(conf[\"module\"], **conf.get(\"args\", {}))\n",
    "            self.edges[name] = conf[\"from\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {self.input_name: x}\n",
    "        for name, layer in self.layers.items():\n",
    "            input_names = self.edges[name]\n",
    "            # 多输入 concat\n",
    "            inputs = [outputs[i] for i in input_names]\n",
    "            if len(inputs) == 1:\n",
    "                out = layer(inputs[0])\n",
    "            else:\n",
    "                # 默认 dim=1 concat\n",
    "                out = layer(torch.cat(inputs, dim=1))\n",
    "            outputs[name] = out\n",
    "        # 返回最后一层输出\n",
    "        return out\n",
    "\n",
    "# --------------------------\n",
    "# 测试\n",
    "# --------------------------\n",
    "layers_config = {\n",
    "    \"conv1\": {\"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\": 3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\": [\"input\"]},\n",
    "    \"relu1\": {\"module\": \"torch.nn.ReLU\", \"args\": {}, \"from\": [\"conv1\"]},\n",
    "    \"conv2\": {\"module\": \"torch.nn.Conv2d\", \"args\": {\"in_channels\":16, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\": [\"relu1\"]},\n",
    "    \"flatten\": {\"module\": \"torch.nn.Flatten\", \"args\": {}, \"from\": [\"conv2\"]},\n",
    "    \"fc\": {\"module\": \"torch.nn.Linear\", \"args\": {\"in_features\":32*32*32, \"out_features\":10}, \"from\": [\"flatten\"]}\n",
    "}\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "model = DAGNet(layers_config)\n",
    "out = model(x)\n",
    "print(out.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f858df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0665, -0.2501, -0.0827, -0.0027,  0.3577, -0.0009, -0.2966, -0.0463,\n",
      "         -0.1123,  0.3288]], grad_fn=<AddmmBackward0>),)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DAGNet(nn.Module):\n",
    "    def __init__(self, layers_config):\n",
    "        \"\"\"\n",
    "        layers_config: dict\n",
    "            key: layer name\n",
    "            value: dict {\n",
    "                \"module\": callable or str,\n",
    "                \"args\": dict,\n",
    "                \"from\": list of input layer names\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers_config = layers_config\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.input_names = []\n",
    "        self.output_names = []\n",
    "\n",
    "        # 实例化每一层\n",
    "        for name, cfg in layers_config.items():\n",
    "            module = cfg[\"module\"]\n",
    "            args = cfg.get(\"args\", {})\n",
    "            # 支持直接传 module 类或 str 名\n",
    "            if isinstance(module, str):\n",
    "                module = eval(module)\n",
    "            self.layers[name] = module(**args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: list or tuple of input tensors, 顺序对应 self.input_names\n",
    "        \"\"\"\n",
    "        if not self.input_names:\n",
    "            raise ValueError(\"self.input_names must be set before forward\")\n",
    "\n",
    "        outputs = {}\n",
    "        # 初始化输入\n",
    "        for i, name in enumerate(self.input_names):\n",
    "            outputs[name] = x[i]\n",
    "\n",
    "        # 遍历所有层\n",
    "        for name, cfg in self.layers_config.items():\n",
    "            from_layers = cfg.get(\"from\", [])\n",
    "            layer = self.layers[name]\n",
    "\n",
    "            # 获取输入张量\n",
    "            if len(from_layers) == 0:\n",
    "                inp = outputs[self.input_names[0]]  # 没有指定来源，用第一个输入\n",
    "            elif len(from_layers) == 1:\n",
    "                inp = outputs[from_layers[0]]\n",
    "            else:\n",
    "                # 多输入 concat\n",
    "                inp = torch.cat([outputs[f] for f in from_layers], dim=1)\n",
    "\n",
    "            outputs[name] = layer(inp)\n",
    "\n",
    "        # 返回输出\n",
    "        if not self.output_names:\n",
    "            # 默认返回最后一层\n",
    "            return outputs[name]\n",
    "        else:\n",
    "            return tuple(outputs[name] for name in self.output_names)\n",
    "\n",
    "# --------------------------\n",
    "# 示例：多输入、多输出\n",
    "# --------------------------\n",
    "layers_config = {\n",
    "    \"conv1\": {\"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img1\"]},\n",
    "    \"conv2\": {\"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img2\"]},\n",
    "    \"concat\": {\"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":32, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\":[\"conv1\",\"conv2\"]},\n",
    "    \"flatten\": {\"module\": \"nn.Flatten\", \"args\": {}, \"from\":[\"concat\"]},\n",
    "    \"fc\": {\"module\": \"nn.Linear\", \"args\": {\"in_features\":32*32*32, \"out_features\":10}, \"from\":[\"flatten\"]}\n",
    "}\n",
    "\n",
    "x1 = torch.randn(1,3,32,32)\n",
    "x2 = torch.randn(1,3,32,32)\n",
    "\n",
    "model = DAGNet(layers_config)\n",
    "model.input_names = [\"img1\",\"img2\"]\n",
    "model.output_names = [\"fc\"]\n",
    "\n",
    "out = model([x1, x2])\n",
    "print(out)\n",
    "print(out[0].shape)\n",
    "# print(out[1].shape)\n",
    "# print(out.shape)  # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eea534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.1884, -0.0595, -0.0263,  0.1358, -0.0650, -0.0262,  0.2738,  0.0362,\n",
      "         -0.0309,  0.0726]], grad_fn=<AddmmBackward0>),)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DAGNet(nn.Module):\n",
    "    def __init__(self, layers_config):\n",
    "        \"\"\"\n",
    "        layers_config: list of dict\n",
    "            每个元素是一个层的配置：\n",
    "            {\n",
    "                \"name\": str,   # 层名字\n",
    "                \"module\": callable or str,\n",
    "                \"args\": dict,\n",
    "                \"from\": list of input layer names\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers_config = layers_config\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.input_names = []\n",
    "        self.output_names = []\n",
    "\n",
    "        # 实例化每一层\n",
    "        for cfg in layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            module = cfg[\"module\"]\n",
    "            args = cfg.get(\"args\", {})\n",
    "            # 支持直接传 module 类或 str 名\n",
    "            if isinstance(module, str):\n",
    "                module = eval(module)\n",
    "            self.layers[name] = module(**args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: list or tuple of input tensors, 顺序对应 self.input_names\n",
    "        \"\"\"\n",
    "        if not self.input_names:\n",
    "            raise ValueError(\"self.input_names must be set before forward\")\n",
    "\n",
    "        outputs = {}\n",
    "        # 初始化输入\n",
    "        for i, name in enumerate(self.input_names):\n",
    "            outputs[name] = x[i]\n",
    "\n",
    "        # 遍历所有层（按顺序）\n",
    "        for cfg in self.layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            from_layers = cfg.get(\"from\", [])\n",
    "            layer = self.layers[name]\n",
    "\n",
    "            # 获取输入张量\n",
    "            if len(from_layers) == 0:\n",
    "                inp = outputs[self.input_names[0]]  # 默认用第一个输入\n",
    "            elif len(from_layers) == 1:\n",
    "                inp = outputs[from_layers[0]]\n",
    "            else:\n",
    "                # 多输入 concat\n",
    "                inp = torch.cat([outputs[f] for f in from_layers], dim=1)\n",
    "\n",
    "            outputs[name] = layer(inp)\n",
    "\n",
    "        # 返回输出\n",
    "        if not self.output_names:\n",
    "            # 默认返回最后一层\n",
    "            return outputs[name]\n",
    "        else:\n",
    "            return tuple(outputs[name] for name in self.output_names)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 示例：多输入、多输出\n",
    "# --------------------------\n",
    "layers_config = [\n",
    "    {\"name\":\"conv1\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img1\"]},\n",
    "    {\"name\":\"conv2\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img2\"]},\n",
    "    {\"name\":\"concat\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":32, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\":[\"conv1\",\"conv2\"]},\n",
    "    {\"name\":\"flatten\", \"module\":\"nn.Flatten\", \"args\":{}, \"from\":[\"concat\"]},\n",
    "    {\"name\":\"fc\", \"module\":\"nn.Linear\", \"args\":{\"in_features\":32*32*32, \"out_features\":10}, \"from\":[\"flatten\"]}\n",
    "]\n",
    "\n",
    "x1 = torch.randn(1,3,32,32)\n",
    "x2 = torch.randn(1,3,32,32)\n",
    "\n",
    "model = DAGNet(layers_config)\n",
    "model.input_names = [\"img1\",\"img2\"]\n",
    "model.output_names = [\"fc\"]\n",
    "\n",
    "out = model([x1, x2])\n",
    "print(out)              # tuple，因为 output_names 是列表\n",
    "print(out[0].shape)     # torch.Size([1, 10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f89cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.3555,  0.1723,  0.0846, -0.0868,  0.2804, -0.1375, -0.1838,  0.2455,\n",
      "         -0.1618,  0.3790]], grad_fn=<AddmmBackward0>),)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DAGNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: dict\n",
    "            {\n",
    "                \"input_names\": [...],\n",
    "                \"output_names\": [...],\n",
    "                \"layers\": [\n",
    "                    {\"name\":..., \"module\":..., \"args\":..., \"from\":[...]},\n",
    "                    ...\n",
    "                ]\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_names = config.get(\"input_names\", [])\n",
    "        self.output_names = config.get(\"output_names\", [])\n",
    "        self.layers_config = config[\"layers\"]\n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # 实例化每一层\n",
    "        for cfg in self.layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            module = cfg[\"module\"]\n",
    "            args = cfg.get(\"args\", {})\n",
    "            if isinstance(module, str):\n",
    "                module = eval(module)\n",
    "            self.layers[name] = module(**args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.input_names:\n",
    "            raise ValueError(\"self.input_names must be set before forward\")\n",
    "\n",
    "        outputs = {}\n",
    "        # 初始化输入\n",
    "        for i, name in enumerate(self.input_names):\n",
    "            outputs[name] = x[i]\n",
    "\n",
    "        # 按顺序遍历每层\n",
    "        for cfg in self.layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            from_layers = cfg.get(\"from\", [])\n",
    "            layer = self.layers[name]\n",
    "\n",
    "            if len(from_layers) == 0:\n",
    "                inp = outputs[self.input_names[0]]\n",
    "            elif len(from_layers) == 1:\n",
    "                inp = outputs[from_layers[0]]\n",
    "            else:\n",
    "                inp = torch.cat([outputs[f] for f in from_layers], dim=1)\n",
    "\n",
    "            outputs[name] = layer(inp)\n",
    "\n",
    "        # 返回输出\n",
    "        if not self.output_names:\n",
    "            return outputs[name]\n",
    "        else:\n",
    "            return tuple(outputs[name] for name in self.output_names)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 示例配置\n",
    "# --------------------------\n",
    "model_config = {\n",
    "    \"input_names\": [\"img1\", \"img2\"],\n",
    "    \"output_names\": [\"fc\"],\n",
    "    \"layers\": [\n",
    "        {\"name\":\"conv1\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img1\"]},\n",
    "        {\"name\":\"conv2\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img2\"]},\n",
    "        {\"name\":\"concat\", \"module\":\"nn.Conv2d\", \"args\":{\"in_channels\":32, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\":[\"conv1\",\"conv2\"]},\n",
    "        {\"name\":\"flatten\", \"module\":\"nn.Flatten\", \"args\":{}, \"from\":[\"concat\"]},\n",
    "        {\"name\":\"fc\", \"module\":\"nn.Linear\", \"args\":{\"in_features\":32*32*32, \"out_features\":10}, \"from\":[\"flatten\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "x1 = torch.randn(1,3,32,32)\n",
    "x2 = torch.randn(1,3,32,32)\n",
    "\n",
    "model = DAGNet(model_config)\n",
    "out = model([x1, x2])\n",
    "print(out)\n",
    "print(out[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa07086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出张量: (tensor([[ 0.0771,  0.0575, -0.2854, -0.0025,  0.0203, -0.2220,  0.2000,  0.0918,\n",
      "          0.2136, -0.1407]], grad_fn=<AddmmBackward0>),)\n",
      "输出形状: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DAGNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        config: dict\n",
    "            input_nodes: list of dict {\"name\": str, \"shape\": tuple}\n",
    "            output_nodes: list of dict {\"name\": str, \"shape\": tuple}\n",
    "            layers: list of dict {\n",
    "                \"name\": str,\n",
    "                \"module\": nn.Module class or str,\n",
    "                \"args\": dict,\n",
    "                \"from\": list of input layer names\n",
    "            }\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_nodes = config.get(\"input_nodes\", [])\n",
    "        self.output_nodes = config.get(\"output_nodes\", [])\n",
    "        self.layers_config = config[\"layers\"]\n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # 实例化每一层\n",
    "        for cfg in self.layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            module = cfg[\"module\"]\n",
    "            args = cfg.get(\"args\", {})\n",
    "            if isinstance(module, str):\n",
    "                module = eval(module)\n",
    "            self.layers[name] = module(**args)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        # 初始化输入并检查形状\n",
    "        for i, node in enumerate(self.input_nodes):\n",
    "            inp = x[i]\n",
    "            expected_shape = node.get(\"shape\")\n",
    "            if expected_shape and inp.shape[1:] != expected_shape:\n",
    "                raise ValueError(f\"Input {node['name']} expected shape {expected_shape}, got {inp.shape[1:]}\")\n",
    "            outputs[node[\"name\"]] = inp\n",
    "\n",
    "        # 遍历所有层\n",
    "        for cfg in self.layers_config:\n",
    "            name = cfg[\"name\"]\n",
    "            from_layers = cfg.get(\"from\", [])\n",
    "            layer = self.layers[name]\n",
    "\n",
    "            # 获取输入张量\n",
    "            if len(from_layers) == 0:\n",
    "                inp = outputs[self.input_nodes[0][\"name\"]]\n",
    "            elif len(from_layers) == 1:\n",
    "                inp = outputs[from_layers[0]]\n",
    "            else:\n",
    "                # 多输入 concat\n",
    "                inp = torch.cat([outputs[f] for f in from_layers], dim=1)\n",
    "\n",
    "            # 对 Linear 层自动计算 in_features\n",
    "            if isinstance(layer, nn.Linear) and 'in_features' not in layer.__dict__:\n",
    "                layer.in_features = inp.numel() // inp.shape[0]\n",
    "                layer.weight = nn.Parameter(torch.empty(layer.out_features, layer.in_features))\n",
    "                layer.bias = nn.Parameter(torch.empty(layer.out_features))\n",
    "\n",
    "            outputs[name] = layer(inp)\n",
    "\n",
    "        # 返回输出\n",
    "        if not self.output_nodes:\n",
    "            return outputs[name]\n",
    "        else:\n",
    "            return tuple(outputs[n[\"name\"]] for n in self.output_nodes)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 示例配置\n",
    "# --------------------------\n",
    "config = {\n",
    "    \"input_nodes\": [\n",
    "        {\"name\": \"img1\", \"shape\": (3, 32, 32)},\n",
    "        {\"name\": \"img2\", \"shape\": (3, 32, 32)}\n",
    "    ],\n",
    "    \"output_nodes\": [\n",
    "        {\"name\": \"fc\", \"shape\": (10,)}\n",
    "    ],\n",
    "    \"layers\": [\n",
    "        {\"name\": \"conv1\", \"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img1\"]},\n",
    "        {\"name\": \"conv2\", \"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":3, \"out_channels\":16, \"kernel_size\":3, \"padding\":1}, \"from\":[\"img2\"]},\n",
    "        {\"name\": \"concat\", \"module\": \"nn.Conv2d\", \"args\": {\"in_channels\":32, \"out_channels\":32, \"kernel_size\":3, \"padding\":1}, \"from\":[\"conv1\",\"conv2\"]},\n",
    "        {\"name\": \"flatten\", \"module\": \"nn.Flatten\", \"args\": {}, \"from\":[\"concat\"]},\n",
    "        {\"name\": \"fc\", \"module\": \"nn.Linear\", \"args\": {\"in_features\":32*32*32, \"out_features\":10}, \"from\":[\"flatten\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 测试\n",
    "# --------------------------\n",
    "x1 = torch.randn(1,3,32,32)\n",
    "x2 = torch.randn(1,3,32,32)\n",
    "\n",
    "model = DAGNet(config)\n",
    "out = model([x1, x2])\n",
    "\n",
    "print(\"输出张量:\", out)\n",
    "print(\"输出形状:\", out[0].shape)  # torch.Size([1, 10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
